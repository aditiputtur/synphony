{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3e4abc",
   "metadata": {},
   "source": [
    "**Synphony**\n",
    "\n",
    "Deep Learning Final Project - MSDS Spring Module 2 - 2025\n",
    "\n",
    "Aditi Puttur & Emma Juan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583649e",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984e5f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from miditok import REMI, TokenizerConfig, TokSequence\n",
    "from miditoolkit import MidiFile\n",
    "from symusic import Score\n",
    "\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffaa729",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16329cc0",
   "metadata": {},
   "source": [
    "### LMD: Midi Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c13a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the JSON file\n",
    "with open('/home/emmajuansalazar/deep-learning-project-MSDS/data/data/LMD/md5_to_paths.json', 'r') as file:\n",
    "    md5_to_paths = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "md5_to_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd_catalog = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk('data/LMD/lmd_matched'):\n",
    "    for file in filenames:\n",
    "        full_path = os.path.join(dirpath, file)\n",
    "        if full_path.endswith('.mid'):\n",
    "            lmd_catalog.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21191e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd_catalog.sort()\n",
    "lmd_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48414537",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lmd_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd_catalog_all = {'path': [],\n",
    "                   'MSD_name': [],\n",
    "                   'LMD_name': []}\n",
    "\n",
    "lmd_catalog_all['path'] = lmd_catalog\n",
    "lmd_catalog_all['MSD_name'] = [path.split('/')[-2] for path in lmd_catalog]\n",
    "lmd_catalog_all['LMD_name'] = [path.split('/')[-1].split('.')[-2] for path in lmd_catalog]\n",
    "\n",
    "lmd_df = pd.DataFrame(lmd_catalog_all)\n",
    "lmd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f59a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd_df[\"MSD_name\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3deb6c2",
   "metadata": {},
   "source": [
    "### LMD-matched metadata (MillionSongDataset): The Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5_getters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24546df",
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_catalog = []\n",
    "titles = []\n",
    "artists = []\n",
    "releases = []\n",
    "years = []\n",
    "\n",
    "for dirpath, dirnames, filenames in tqdm(os.walk('data/LMD-matched-MSD')):\n",
    "    for file in filenames:\n",
    "        full_path = os.path.join(dirpath, file)\n",
    "        if full_path.endswith('.h5'):\n",
    "\n",
    "            # Append the path to the list\n",
    "            msd_catalog.append(full_path)\n",
    "\n",
    "            # Get the metadata\n",
    "            h5 = hdf5_getters.open_h5_file_read(full_path)\n",
    "            titles.append(hdf5_getters.get_title(h5))\n",
    "            artists.append(hdf5_getters.get_artist_name(h5))\n",
    "            releases.append(hdf5_getters.get_release(h5))\n",
    "            years.append(hdf5_getters.get_year(h5))\n",
    "            # danceability = hdf5_getters.get_danceability(h5)\n",
    "            # get_energy = hdf5_getters.get_energy(h5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30593c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msd_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0090ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msd_catalog) == lmd_df[\"MSD_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db678fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "years[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [title.decode('utf-8') for title in titles]\n",
    "artists = [artist.decode('utf-8') for artist in artists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_catalog_all = {'path': [],\n",
    "                   'MSD_name': [],\n",
    "                   'title': [],\n",
    "                   'artist': [],\n",
    "                   'year': []}\n",
    "\n",
    "msd_catalog_all['path'] = msd_catalog\n",
    "msd_catalog_all['title'] = titles\n",
    "msd_catalog_all['artist'] = artists\n",
    "msd_catalog_all['year'] = years\n",
    "msd_catalog_all['MSD_name'] = [path.split('/')[-1].split('.')[-2] for path in msd_catalog]\n",
    "\n",
    "msd_df = pd.DataFrame(msd_catalog_all)\n",
    "msd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf44bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016340fe",
   "metadata": {},
   "source": [
    "### tagtraum: Adding Genre Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50611292",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagtraum = {'MSD_name': [],\n",
    "            'genre': []}\n",
    "\n",
    "with open(\"data/tagtraum/msd_tagtraum_cd2c.cls\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            track, genre = line.strip().split('\\t')\n",
    "            tagtraum['MSD_name'].append(track)\n",
    "            tagtraum['genre'].append(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760539ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagtraum_df = pd.DataFrame(tagtraum)\n",
    "tagtraum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b943289",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagtraum_df[\"genre\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ba284",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Creating our dataset: MIDI + Metadata + Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1b4a2",
   "metadata": {},
   "source": [
    "### Midi + Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ac540",
   "metadata": {},
   "source": [
    "**Each track (MSD_name -> track_id) has one metadata file, and different MIDI files (LMD_name -> midi_id) associated with it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lmd_df), len(msd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd_df[\"MSD_name\"].nunique(), len(msd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lmd_df.merge(msd_df, how=\"inner\", on=\"MSD_name\", suffixes=('_lmd', '_msd'))\n",
    "dataset = dataset.rename(columns={\"path_lmd\": \"midi_filepath\",\n",
    "                                  \"path_msd\": \"metadata_filepath\",\n",
    "                                  \"MSD_name\": \"track_id\",\n",
    "                                  \"LMD_name\": \"midi_id\"})\n",
    "dataset = dataset[[\"track_id\", \"midi_id\", \"midi_filepath\",\n",
    "                   \"title\", \"artist\", \"year\"]]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd80d1",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "grouped_dataset = dataset.groupby('track_id').first().reset_index()\n",
    "grouped_dataset = grouped_dataset[['track_id', 'midi_id', 'midi_filepath']]\n",
    "grouped_dataset = grouped_dataset.merge(\n",
    "    dataset[\n",
    "        ['track_id', \"title\", \"artist\", \"year\"]\n",
    "    ].drop_duplicates(), on='track_id', how='left' )\n",
    "grouped_dataset = grouped_dataset[[\"track_id\", \"midi_id\", \"midi_filepath\",\n",
    "                                   \"title\", \"artist\", \"year\"]]\n",
    "grouped_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7892095",
   "metadata": {},
   "source": [
    "### Adding the genre tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(tagtraum_df, how=\"inner\", left_on=\"track_id\", right_on=\"MSD_name\")\n",
    "dataset = dataset.drop(columns=[\"MSD_name\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dataset = grouped_dataset.merge(tagtraum_df, how=\"inner\", left_on=\"track_id\", right_on=\"MSD_name\")\n",
    "grouped_dataset = grouped_dataset.drop(columns=[\"MSD_name\"])\n",
    "grouped_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c3fb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sluggifying our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = dataset[\"genre\"].unique()\n",
    "artists = dataset[\"artist\"].unique()\n",
    "years = dataset[\"year\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slug(text: str) -> str:\n",
    "    \"\"\"Return an ALL_CAPS alnum/underscore version of `text`.\"\"\"\n",
    "    # 1) strip accents → ascii\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode()\n",
    "    # 2) replace non‑alnum with underscore\n",
    "    text = re.sub(r\"[^\\w]+\", \"_\", text)\n",
    "    # 3) collapse multiple underscores and upper‑case\n",
    "    return re.sub(r\"_+\", \"_\", text).strip(\"_\").upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_slugged = np.array([slug(genre) for genre in genres])\n",
    "artists_slugged = np.array([slug(artist) for artist in artists])\n",
    "years = np.array([int(year) for year in years if not pd.isna(year)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = pd.DataFrame({\n",
    "    'genre': genres,\n",
    "    'slugged_genre': genres_slugged\n",
    "})\n",
    "\n",
    "artists = pd.DataFrame({\n",
    "    'artist': artists,\n",
    "    'slugged_artist': artists_slugged\n",
    "})\n",
    "\n",
    "years = pd.DataFrame({\n",
    "    'year': years\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b674d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = genres.sort_values(by='genre')\n",
    "artists = artists.sort_values(by='artist')\n",
    "years = years.sort_values(by='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ccfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"slugged_genre\"] = dataset[\"genre\"].map(genres.set_index('genre')['slugged_genre'])\n",
    "dataset[\"slugged_artist\"] = dataset[\"artist\"].map(artists.set_index('artist')['slugged_artist'])\n",
    "\n",
    "grouped_dataset[\"slugged_genre\"] = grouped_dataset[\"genre\"].map(genres.set_index('genre')['slugged_genre'])\n",
    "grouped_dataset[\"slugged_artist\"] = grouped_dataset[\"artist\"].map(artists.set_index('artist')['slugged_artist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b40cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Saving our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace99ee8",
   "metadata": {},
   "source": [
    "### Saving the metadata datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"data/metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab276c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dataset.to_csv(\"data/grouped_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb416e1",
   "metadata": {},
   "source": [
    "### Saving the different parameters to csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres.to_csv(\"data/genres.csv\", index=False)\n",
    "artists.to_csv(\"data/artists.csv\", index=False)\n",
    "years.to_csv(\"data/years.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7a874",
   "metadata": {},
   "source": [
    "# 2. Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ded7419c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/home/emmajuansalazar/deep-learning-project-MSDS/data/data/metadata.csv\")\n",
    "grouped_dataset = pd.read_csv(\"/home/emmajuansalazar/deep-learning-project-MSDS/data/data/grouped_metadata.csv\")\n",
    "\n",
    "genres = pd.read_csv(\"/home/emmajuansalazar/deep-learning-project-MSDS/data/data/genres.csv\")\n",
    "titles = pd.read_csv(\"/home/emmajuansalazar/deep-learning-project-MSDS/data/data/titles.csv\")\n",
    "artists = pd.read_csv(\"/home/emmajuansalazar/deep-learning-project-MSDS/data/data/artists.csv\")\n",
    "years = pd.read_csv(\"/home/emmajuansalazar/deep-learning-project-MSDS/data/data/years.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee859144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "genres_slugged = genres[\"slugged_genre\"].values\n",
    "artists_slugged = artists[\"slugged_artist\"].values\n",
    "years_vals = years[\"year\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbd835ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Config whith which the model was trained\n",
    "# MAX_TOKENS = 128\n",
    "# BATCH_SIZE = 1\n",
    "\n",
    "# D_MODEL    = 128\n",
    "# N_LAYERS   = 1\n",
    "# N_HEADS    = 1\n",
    "\n",
    "# New config to try\n",
    "MAX_TOKENS = 512\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "D_MODEL = 512\n",
    "N_LAYERS = 6\n",
    "N_HEADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842c743",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36dc3e",
   "metadata": {},
   "source": [
    "### Defining the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aad7d08c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config = TokenizerConfig(\n",
    "#     pitch_range=(21, 108),           # A0–C8\n",
    "#     beat_res={(0, 4): 8, (4, 8): 4}, # finer grid in 1st half‑bar\n",
    "#     num_velocities=32,\n",
    "#     use_rests=True,\n",
    "#     rest_range=(2, 8),               # long rests allowed\n",
    "#     use_tempos=True,\n",
    "#     use_chords=False,\n",
    "#     use_time_signatures=False,\n",
    "#     # you can still add / remove special tokens later with\n",
    "#     # tokenizer.add_to_vocab([...])\n",
    "# )\n",
    "config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "\n",
    "tokenizer = REMI(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ffaa7",
   "metadata": {},
   "source": [
    "### Adding our special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ae9165b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "special_toks = \\\n",
    "    [f\"<GENRE_{g}>\"  for g in genres_slugged] + \\\n",
    "        [f\"<ARTIST_{a}>\" for a in artists_slugged] + \\\n",
    "            [f\"<YEAR_{y}>\"   for y in years_vals]  + \\\n",
    "                [\"<EOS>\", \"<PAD>\"]\n",
    "\n",
    "for tok in special_toks:\n",
    "    tokenizer.add_to_vocab(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638cbf00",
   "metadata": {},
   "source": [
    "### Tokenizing: Storing each track as a numpy int32 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeb3ed4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47b889b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ─── 1. Helpers ──────────────────────────────────────────────────────────\n",
    "def build_prefix(genre, artist, year, tokenizer):\n",
    "    \"\"\"Convert metadata row → list[int] conditioning tokens.\"\"\"\n",
    "    genre_tok  = f\"<GENRE_{genre}>\"\n",
    "    artist_tok = f\"<ARTIST_{artist}>\"\n",
    "    year_tok   = f\"<YEAR_{year}>\"\n",
    "\n",
    "    # NOTE: use tokenizer.vocab[...]  (or .token_to_id(...))\n",
    "    return [\n",
    "        tokenizer.vocab[genre_tok],\n",
    "        tokenizer.vocab[artist_tok],\n",
    "        tokenizer.vocab[year_tok],\n",
    "    ]\n",
    "\n",
    "# ─── 3. Output directory -------------------------------------------------\n",
    "out_dir = \"data/tokens/train\"\n",
    "\n",
    "# ─── 4. Iterate files ----------------------------------------------------\n",
    "if tokenizing:\n",
    "    rows, _ = grouped_dataset.shape\n",
    "    for row in tqdm(range(1000)):\n",
    "        try:\n",
    "            # 4.0. Get row\n",
    "            row = grouped_dataset.iloc[row]\n",
    "\n",
    "            # 4.1. Get MIDI filepath\n",
    "            midi_path = row[\"midi_filepath\"]\n",
    "\n",
    "            # 4.2. Get the track ID\n",
    "            track_id = row[\"track_id\"]\n",
    "\n",
    "            # 4a. Build CONDITIONING prefix\n",
    "            genre = row[\"slugged_genre\"]\n",
    "            artist = row[\"slugged_artist\"]\n",
    "            year = row[\"year\"]\n",
    "            prefix_ids = build_prefix(genre, artist, year, tokenizer)          # list[int]\n",
    "\n",
    "            # 4b. Encode MIDI to tokens\n",
    "            midi = Score(midi_path)\n",
    "            midi_tokens = tokenizer(midi)                 # list[int]\n",
    "\n",
    "            # 4c. Concatenate prefix + midi + <EOS>\n",
    "            seq_ids = prefix_ids + midi_tokens.ids + [tokenizer.vocab[\"<EOS>\"]]\n",
    "\n",
    "            # 4d. Save as int32 .npy\n",
    "            np.save(f\"{out_dir}/{track_id}.npy\", np.array(seq_ids, dtype=np.int32))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {midi_path}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff6d76",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "991458c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RelativePositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal *relative‑style* positional encoding.\n",
    "    The tensor it returns has the same shape as `x`\n",
    "    so you can just add it:  x + pos(x)\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    d_model : int            # embedding size\n",
    "    max_len : int, optional  # maximum sequence length\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Create the (max_len, d_model) sinusoid table once\n",
    "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float)\n",
    "            * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)          # (L, D)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Register as a buffer so it moves with .to(device)\n",
    "        self.register_buffer(\"pe\", pe)              # (L, D)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor, shape (batch, seq_len, d_model)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pos : Tensor, same shape as `x`\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds max_len {self.max_len}\")\n",
    "        # (1, L, D) – broadcast over batch dimension\n",
    "        return self.pe[:seq_len].unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c482150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block that merges causal + pad masking into a (B×H, L, L) float mask,\n",
    "    so no hidden bool→float blow-ups occur.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        max_len: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim   = d_model,\n",
    "            num_heads   = n_heads,\n",
    "            dropout     = dropout,\n",
    "            batch_first = True,\n",
    "        )\n",
    "        self.ln1      = nn.LayerNorm(d_model)\n",
    "        self.ln2      = nn.LayerNorm(d_model)\n",
    "        self.ff       = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "\n",
    "        # Precompute float causal mask: 0 on/under diag, -inf above\n",
    "        causal = torch.triu(\n",
    "            torch.full((max_len, max_len), float(\"-inf\")),\n",
    "            diagonal=1\n",
    "        )\n",
    "        self.register_buffer(\"causal_mask\", causal, persistent=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,            # (B, L, D)\n",
    "        pad_mask: torch.Tensor=None  # (B, L), True=keep token, False=pad\n",
    "    ) -> torch.Tensor:\n",
    "        B, L, _ = x.shape\n",
    "        H       = self.self_attn.num_heads\n",
    "        device  = x.device\n",
    "        dtype   = x.dtype\n",
    "\n",
    "        # 1) slice the (L×L) causal mask\n",
    "        causal = self.causal_mask[:L, :L]              # float32, (L, L)\n",
    "\n",
    "        # 2) build a (B, L) float pad mask: 0 on tokens, -inf on pads\n",
    "        if pad_mask is not None:\n",
    "            pad_float = torch.zeros((B, L), device=device, dtype=dtype)\n",
    "            pad_float = pad_float.masked_fill(~pad_mask, float(\"-inf\"))\n",
    "            # 3) expand pad_float to (B, L, L) and add causal\n",
    "            #    pad_float.unsqueeze(1): (B, 1, L) → broadcast over src_len\n",
    "            attn_batch = causal.unsqueeze(0) + pad_float.unsqueeze(1)  # (B, L, L)\n",
    "        else:\n",
    "            attn_batch = causal                               # (L, L)\n",
    "\n",
    "        # 4) if we have a batch, repeat per-head to (B×H, L, L)\n",
    "        if pad_mask is not None:\n",
    "            # attn_batch: (B, L, L) → repeat each batch H times\n",
    "            attn_mask = attn_batch.repeat_interleave(H, dim=0)  # (B*H, L, L)\n",
    "        else:\n",
    "            attn_mask = attn_batch   # 2D mask\n",
    "\n",
    "        # 5) self-attention with ONLY attn_mask\n",
    "        attn_out, _ = self.self_attn(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "\n",
    "        # 6) residual + norm + feed-forward + norm\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        x = self.ln2(x + self.dropout(self.ff(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdd572fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Synphony(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, n_layers=6, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = RelativePositionalEncoding(d_model, max_len=2048)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerDecoderBlock(d_model, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.embed(x) + self.pos(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, pad_mask)\n",
    "        x = self.ln(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1fddf",
   "metadata": {},
   "source": [
    "## The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc5ec0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "random.seed(42)  # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11aca1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tok_paths = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk('/home/emmajuansalazar/deep-learning-project-MSDS/data/data/tokens/train'):\n",
    "    for file in filenames:\n",
    "        full_path = os.path.join(dirpath, file)\n",
    "        if full_path.endswith('.npy'):\n",
    "            tok_paths.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0cc6d49f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_index = int(len(tok_paths) * 0.8)  # 80% train, 20% test\n",
    "random.shuffle(tok_paths)\n",
    "\n",
    "train_paths = tok_paths[:split_index]\n",
    "test_paths = tok_paths[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37dbd638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val PPL 217.33\n",
      "Epoch 01 ▸ train PPL 712.98 | val PPL 217.33\n",
      "  ✓ new best model saved\n",
      "val PPL  59.08\n",
      "Epoch 02 ▸ train PPL 164.65 | val PPL  59.08\n",
      "  ✓ new best model saved\n",
      "val PPL  27.55\n",
      "Epoch 03 ▸ train PPL  68.55 | val PPL  27.55\n",
      "  ✓ new best model saved\n",
      "val PPL  18.54\n",
      "Epoch 04 ▸ train PPL  43.19 | val PPL  18.54\n",
      "  ✓ new best model saved\n",
      "val PPL  15.08\n",
      "Epoch 05 ▸ train PPL  34.58 | val PPL  15.08\n",
      "  ✓ new best model saved\n",
      "val PPL  13.51\n",
      "Epoch 06 ▸ train PPL  30.45 | val PPL  13.51\n",
      "  ✓ new best model saved\n",
      "val PPL  12.04\n",
      "Epoch 07 ▸ train PPL  27.86 | val PPL  12.04\n",
      "  ✓ new best model saved\n",
      "val PPL  11.63\n",
      "Epoch 08 ▸ train PPL  26.18 | val PPL  11.63\n",
      "  ✓ new best model saved\n",
      "val PPL  10.65\n",
      "Epoch 09 ▸ train PPL  24.46 | val PPL  10.65\n",
      "  ✓ new best model saved\n",
      "val PPL  10.41\n",
      "Epoch 10 ▸ train PPL  23.24 | val PPL  10.41\n",
      "  ✓ new best model saved\n",
      "val PPL   9.30\n",
      "Epoch 11 ▸ train PPL  21.85 | val PPL   9.30\n",
      "  ✓ new best model saved\n",
      "val PPL   8.54\n",
      "Epoch 12 ▸ train PPL  19.81 | val PPL   8.54\n",
      "  ✓ new best model saved\n",
      "val PPL   8.00\n",
      "Epoch 13 ▸ train PPL  18.64 | val PPL   8.00\n",
      "  ✓ new best model saved\n",
      "val PPL   7.49\n",
      "Epoch 14 ▸ train PPL  17.53 | val PPL   7.49\n",
      "  ✓ new best model saved\n",
      "val PPL   7.02\n",
      "Epoch 15 ▸ train PPL  16.47 | val PPL   7.02\n",
      "  ✓ new best model saved\n",
      "val PPL   6.47\n",
      "Epoch 16 ▸ train PPL  15.52 | val PPL   6.47\n",
      "  ✓ new best model saved\n",
      "val PPL   6.23\n",
      "Epoch 17 ▸ train PPL  14.69 | val PPL   6.23\n",
      "  ✓ new best model saved\n",
      "val PPL   5.81\n",
      "Epoch 18 ▸ train PPL  13.74 | val PPL   5.81\n",
      "  ✓ new best model saved\n",
      "val PPL   5.49\n",
      "Epoch 19 ▸ train PPL  13.24 | val PPL   5.49\n",
      "  ✓ new best model saved\n",
      "val PPL   5.11\n",
      "Epoch 20 ▸ train PPL  12.62 | val PPL   5.11\n",
      "  ✓ new best model saved\n",
      "val PPL   4.83\n",
      "Epoch 21 ▸ train PPL  12.14 | val PPL   4.83\n",
      "  ✓ new best model saved\n",
      "val PPL   4.76\n",
      "Epoch 22 ▸ train PPL  11.77 | val PPL   4.76\n",
      "  ✓ new best model saved\n",
      "val PPL   4.61\n",
      "Epoch 23 ▸ train PPL  11.35 | val PPL   4.61\n",
      "  ✓ new best model saved\n",
      "val PPL   4.63\n",
      "Epoch 24 ▸ train PPL  11.02 | val PPL   4.63\n",
      "val PPL   4.33\n",
      "Epoch 25 ▸ train PPL  10.72 | val PPL   4.33\n",
      "  ✓ new best model saved\n",
      "val PPL   4.35\n",
      "Epoch 26 ▸ train PPL  10.48 | val PPL   4.35\n",
      "val PPL   4.29\n",
      "Epoch 27 ▸ train PPL  10.33 | val PPL   4.29\n",
      "  ✓ new best model saved\n",
      "val PPL   4.06\n",
      "Epoch 28 ▸ train PPL  10.05 | val PPL   4.06\n",
      "  ✓ new best model saved\n",
      "val PPL   4.12\n",
      "Epoch 29 ▸ train PPL  10.01 | val PPL   4.12\n",
      "val PPL   4.03\n",
      "Epoch 30 ▸ train PPL   9.74 | val PPL   4.03\n",
      "  ✓ new best model saved\n",
      "val PPL   3.95\n",
      "Epoch 31 ▸ train PPL   9.80 | val PPL   3.95\n",
      "  ✓ new best model saved\n",
      "val PPL   3.97\n",
      "Epoch 32 ▸ train PPL   9.67 | val PPL   3.97\n",
      "val PPL   3.94\n",
      "Epoch 33 ▸ train PPL   9.51 | val PPL   3.94\n",
      "  ✓ new best model saved\n",
      "val PPL   3.85\n",
      "Epoch 34 ▸ train PPL   9.43 | val PPL   3.85\n",
      "  ✓ new best model saved\n",
      "val PPL   3.92\n",
      "Epoch 35 ▸ train PPL   9.33 | val PPL   3.92\n",
      "val PPL   3.86\n",
      "Epoch 36 ▸ train PPL   9.19 | val PPL   3.86\n",
      "val PPL   3.84\n",
      "Epoch 37 ▸ train PPL   9.16 | val PPL   3.84\n",
      "  ✓ new best model saved\n",
      "val PPL   3.81\n",
      "Epoch 38 ▸ train PPL   9.05 | val PPL   3.81\n",
      "  ✓ new best model saved\n",
      "val PPL   3.82\n",
      "Epoch 39 ▸ train PPL   9.03 | val PPL   3.82\n",
      "val PPL   3.85\n",
      "Epoch 40 ▸ train PPL   8.90 | val PPL   3.85\n",
      "val PPL   3.75\n",
      "Epoch 41 ▸ train PPL   8.88 | val PPL   3.75\n",
      "  ✓ new best model saved\n",
      "val PPL   3.72\n",
      "Epoch 42 ▸ train PPL   8.89 | val PPL   3.72\n",
      "  ✓ new best model saved\n",
      "val PPL   3.75\n",
      "Epoch 43 ▸ train PPL   8.84 | val PPL   3.75\n",
      "val PPL   3.67\n",
      "Epoch 44 ▸ train PPL   8.85 | val PPL   3.67\n",
      "  ✓ new best model saved\n",
      "val PPL   3.72\n",
      "Epoch 45 ▸ train PPL   8.74 | val PPL   3.72\n",
      "val PPL   3.70\n",
      "Epoch 46 ▸ train PPL   8.77 | val PPL   3.70\n",
      "val PPL   3.70\n",
      "Epoch 47 ▸ train PPL   8.77 | val PPL   3.70\n",
      "val PPL   3.75\n",
      "Epoch 48 ▸ train PPL   8.71 | val PPL   3.75\n",
      "val PPL   3.78\n",
      "Epoch 49 ▸ train PPL   8.66 | val PPL   3.78\n",
      "val PPL   3.67\n",
      "Epoch 50 ▸ train PPL   8.79 | val PPL   3.67\n",
      "  ✓ new best model saved\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# ─── 1. Dataset + collate ────────────────────────────────────────────────\n",
    "class MidiTokenDataset(Dataset):\n",
    "    def __init__(self, npy_paths):\n",
    "        self.paths = npy_paths\n",
    "\n",
    "    def __len__(self):               # number of songs in split\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):      # returns 1‑D np.ndarray[int]\n",
    "        return np.load(self.paths[idx]).astype(np.int64)\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    B, L = len(batch), MAX_TOKENS\n",
    "    x = torch.full((B, L), pad_id, dtype=torch.long)\n",
    "    for i, seq in enumerate(batch):\n",
    "        seq = torch.from_numpy(seq)\n",
    "        if seq.numel() > L:\n",
    "            start = torch.randint(0, seq.numel() - L + 1, (1,)).item()\n",
    "            seq = seq[start : start + L]\n",
    "        x[i, : seq.numel()] = seq\n",
    "    pad_mask = ~x.eq(pad_id)\n",
    "    return x, pad_mask\n",
    "\n",
    "\n",
    "# ─── 2. DataLoaders ──────────────────────────────────────────────────────\n",
    "PAD_ID = tokenizer.vocab['<PAD>']          # or use the ID you chose for <PAD>\n",
    "\n",
    "train_ds = MidiTokenDataset(train_paths)\n",
    "val_ds   = MidiTokenDataset(test_paths)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=lambda b: collate_fn(b, PAD_ID)\n",
    ")\n",
    "val_loader   = DataLoader(\n",
    "    val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "    collate_fn=lambda b: collate_fn(b, PAD_ID)\n",
    ")\n",
    "\n",
    "# ─── 3. Model, optimiser, scheduler ─────────────────────────────────────\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "model = Synphony(\n",
    "    vocab_size=len(tokenizer), d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS, n_heads=N_HEADS).to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optim, max_lr=1e-4,\n",
    "    steps_per_epoch=len(train_loader), epochs=50\n",
    ")\n",
    "\n",
    "\n",
    "# ─── 4. Training loop ────────────────────────────────────────────────────\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, 51):                         # 50 epochs\n",
    "    # ---- train ----------------------------------------------------------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, pad_mask in train_loader:          # pad_mask: (B, L)\n",
    "        x, pad_mask = x.to(device), pad_mask.to(device)\n",
    "\n",
    "        logits = model(x[:, :-1], pad_mask=pad_mask[:, :-1])\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            x[:, 1:].reshape(-1),\n",
    "            ignore_index=PAD_ID,\n",
    "            label_smoothing=0.1\n",
    "        )\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step(); sched.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_ppl = math.exp(running_loss / len(train_loader))\n",
    "\n",
    "    # ---- validation -----------------------------------------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, pad_mask in val_loader:             # pad_mask is (B, L)\n",
    "            x, pad_mask = x.to(device), pad_mask.to(device)\n",
    "\n",
    "            # exactly like in training\n",
    "            logits  = model(x[:, :-1], pad_mask=pad_mask[:, :-1])\n",
    "            val_loss += F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                x[:, 1:].reshape(-1),\n",
    "                ignore_index=PAD_ID\n",
    "            ).item()\n",
    "\n",
    "    val_ppl = math.exp(val_loss / len(val_loader))\n",
    "    print(f\"val PPL {val_ppl:6.2f}\")\n",
    "    print(f\"Epoch {epoch:02d} ▸ train PPL {train_ppl:6.2f} | val PPL {val_ppl:6.2f}\")\n",
    "\n",
    "    # ---- checkpoint -----------------------------------------------------\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"synphony_best.pt\")\n",
    "        print(\"  ✓ new best model saved\")\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c49a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02358a98",
   "metadata": {},
   "source": [
    "# 3. Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfd5b917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synphony(\n",
       "  (embed): Embedding(3429, 128)\n",
       "  (pos): RelativePositionalEncoding()\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerDecoderBlock(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=128, out_features=3429, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 1.0\n",
    "TOP_K = 8\n",
    "\n",
    "# ─── 2. Helper for top-k filtering ───────────────────────────────────────\n",
    "def top_k_logits(logits, k):\n",
    "    v, _ = torch.topk(logits, k)\n",
    "    threshold = v[-1]\n",
    "    return torch.where(logits < threshold, torch.full_like(logits, -float(\"Inf\")), logits)\n",
    "\n",
    "# ─── 3. Autoregressive generation ────────────────────────────────────────\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "        genre:str,\n",
    "        artist:str,\n",
    "        year:int,\n",
    "        max_length:int = MAX_TOKENS\n",
    "    ) -> list[int]:\n",
    "    prefix = build_prefix(genre, artist, year, tokenizer)\n",
    "    input_ids = torch.tensor([prefix], device=device)  # (1, P)\n",
    "    pad_mask  = torch.ones_like(input_ids, dtype=torch.bool, device=device)\n",
    "\n",
    "    for _ in tqdm(range(max_length - len(prefix))):\n",
    "        logits = model(input_ids, pad_mask=pad_mask)\n",
    "        next_logits = logits[0, -1, :]                  # (V,)\n",
    "        next_logits = next_logits / TEMPERATURE\n",
    "        next_logits = top_k_logits(next_logits, TOP_K)\n",
    "        probs = F.softmax(next_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)  # (1,)\n",
    "        if next_id.item() == tokenizer.vocab[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "        # append and extend pad_mask\n",
    "        input_ids = torch.cat([input_ids, next_id.unsqueeze(0)], dim=1)   # (1, L+1)\n",
    "        pad_mask  = torch.ones_like(input_ids, dtype=torch.bool, device=device)\n",
    "\n",
    "    return input_ids[0].tolist()\n",
    "\n",
    "# ─── 4. Decode to MIDI & save ────────────────────────────────────────────\n",
    "def tokens_to_midi(token_ids: list[int], out_path: str):\n",
    "    \"\"\"\n",
    "    Drop the 3 metadata tokens + optional EOS, then decode the rest.\n",
    "    \"\"\"\n",
    "    # 1) drop the first 3 prefix IDs (genre, artist, year)\n",
    "    musical_ids = token_ids[3:]\n",
    "    # 2) drop trailing <EOS> if present\n",
    "    eos_id = tokenizer.vocab[\"<EOS>\"]\n",
    "    if len(musical_ids) > 0 and musical_ids[-1] == eos_id:\n",
    "        musical_ids = musical_ids[:-1]\n",
    "\n",
    "    # 3) decode only the musical tokens back to a PrettyMIDI\n",
    "    pm = tokenizer(musical_ids)\n",
    "    # 4) write out the .mid file\n",
    "    pm.dump_midi(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6fd5313",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m artist_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRICK_ASTLEY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m year_input   \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1987\u001b[39m\n\u001b[0;32m----> 7\u001b[0m gen_ids \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m(genre_input, artist_input, year_input)\n\u001b[1;32m      8\u001b[0m out_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated.mid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m tokens_to_midi(gen_ids, out_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate' is not defined"
     ]
    }
   ],
   "source": [
    "# ─── 5. Run it! ───────────────────────────────────────────────────────────\n",
    "# Example user inputs\n",
    "genre_input  = \"POP\"\n",
    "artist_input = \"RICK_ASTLEY\"\n",
    "year_input   = 1987\n",
    "\n",
    "gen_ids = generate(genre_input, artist_input, year_input)\n",
    "out_file = \"generated.mid\"\n",
    "tokens_to_midi(gen_ids, out_file)\n",
    "print(f\"🎹 Wrote MIDI to {out_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9eb0d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-2.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-2:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
