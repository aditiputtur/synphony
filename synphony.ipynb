{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3e4abc",
   "metadata": {},
   "source": [
    "**Synphony**\n",
    "\n",
    "Deep Learning Final Project - MSDS Spring Module 2 - 2025\n",
    "\n",
    "Aditi Puttur & Emma Juan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583649e",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984e5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from miditok import REMI, TokenizerConfig, TokSequence\n",
    "from miditoolkit import MidiFile\n",
    "from symusic import Score\n",
    "\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffaa729",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16329cc0",
   "metadata": {},
   "source": [
    "### LMD: Midi Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c13a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and read the JSON file\n",
    "with open('data/LMD/md5_to_paths.json', 'r') as file:\n",
    "    md5_to_paths = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "md5_to_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dcc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd_catalog = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk('data/LMD/lmd_matched'):\n",
    "    for file in filenames:\n",
    "        full_path = os.path.join(dirpath, file)\n",
    "        if full_path.endswith('.mid'):\n",
    "            lmd_catalog.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21191e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd_catalog.sort()\n",
    "lmd_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48414537",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lmd_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd_catalog_all = {'path': [],\n",
    "                   'MSD_name': [],\n",
    "                   'LMD_name': []}\n",
    "\n",
    "lmd_catalog_all['path'] = lmd_catalog\n",
    "lmd_catalog_all['MSD_name'] = [path.split('/')[-2] for path in lmd_catalog]\n",
    "lmd_catalog_all['LMD_name'] = [path.split('/')[-1].split('.')[-2] for path in lmd_catalog]\n",
    "\n",
    "lmd_df = pd.DataFrame(lmd_catalog_all)\n",
    "lmd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f59a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd_df[\"MSD_name\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3deb6c2",
   "metadata": {},
   "source": [
    "### LMD-matched metadata (MillionSongDataset): The Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5_getters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24546df",
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_catalog = []\n",
    "titles = []\n",
    "artists = []\n",
    "releases = []\n",
    "years = []\n",
    "\n",
    "for dirpath, dirnames, filenames in tqdm(os.walk('data/LMD-matched-MSD')):\n",
    "    for file in filenames:\n",
    "        full_path = os.path.join(dirpath, file)\n",
    "        if full_path.endswith('.h5'):\n",
    "\n",
    "            # Append the path to the list\n",
    "            msd_catalog.append(full_path)\n",
    "\n",
    "            # Get the metadata\n",
    "            h5 = hdf5_getters.open_h5_file_read(full_path)\n",
    "            titles.append(hdf5_getters.get_title(h5))\n",
    "            artists.append(hdf5_getters.get_artist_name(h5))\n",
    "            releases.append(hdf5_getters.get_release(h5))\n",
    "            years.append(hdf5_getters.get_year(h5))\n",
    "            # danceability = hdf5_getters.get_danceability(h5)\n",
    "            # get_energy = hdf5_getters.get_energy(h5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30593c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msd_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0090ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msd_catalog) == lmd_df[\"MSD_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db678fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "years[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [title.decode('utf-8') for title in titles]\n",
    "artists = [artist.decode('utf-8') for artist in artists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_catalog_all = {'path': [],\n",
    "                   'MSD_name': [],\n",
    "                   'title': [],\n",
    "                   'artist': [],\n",
    "                   'year': []}\n",
    "\n",
    "msd_catalog_all['path'] = msd_catalog\n",
    "msd_catalog_all['title'] = titles\n",
    "msd_catalog_all['artist'] = artists\n",
    "msd_catalog_all['year'] = years\n",
    "msd_catalog_all['MSD_name'] = [path.split('/')[-1].split('.')[-2] for path in msd_catalog]\n",
    "\n",
    "msd_df = pd.DataFrame(msd_catalog_all)\n",
    "msd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf44bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016340fe",
   "metadata": {},
   "source": [
    "### tagtraum: Adding Genre Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50611292",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagtraum = {'MSD_name': [],\n",
    "            'genre': []}\n",
    "\n",
    "with open(\"data/tagtraum/msd_tagtraum_cd2c.cls\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        if not line.startswith('#'):\n",
    "            track, genre = line.strip().split('\\t')\n",
    "            tagtraum['MSD_name'].append(track)\n",
    "            tagtraum['genre'].append(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760539ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagtraum_df = pd.DataFrame(tagtraum)\n",
    "tagtraum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b943289",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagtraum_df[\"genre\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ba284",
   "metadata": {},
   "source": [
    "## Creating our dataset: MIDI + Metadata + Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1b4a2",
   "metadata": {},
   "source": [
    "### Midi + Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ac540",
   "metadata": {},
   "source": [
    "**Each track (MSD_name -> track_id) has one metadata file, and different MIDI files (LMD_name -> midi_id) associated with it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lmd_df), len(msd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmd_df[\"MSD_name\"].nunique(), len(msd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lmd_df.merge(msd_df, how=\"inner\", on=\"MSD_name\", suffixes=('_lmd', '_msd'))\n",
    "dataset = dataset.rename(columns={\"path_lmd\": \"midi_filepath\",\n",
    "                                  \"path_msd\": \"metadata_filepath\",\n",
    "                                  \"MSD_name\": \"track_id\",\n",
    "                                  \"LMD_name\": \"midi_id\"})\n",
    "dataset = dataset[[\"track_id\", \"midi_id\", \"midi_filepath\",\n",
    "                   \"title\", \"artist\", \"year\"]]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd80d1",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "grouped_dataset = dataset.groupby('track_id').first().reset_index()\n",
    "grouped_dataset = grouped_dataset[['track_id', 'midi_id', 'midi_filepath']]\n",
    "grouped_dataset = grouped_dataset.merge(\n",
    "    dataset[\n",
    "        ['track_id', \"title\", \"artist\", \"year\"]\n",
    "    ].drop_duplicates(), on='track_id', how='left' )\n",
    "grouped_dataset = grouped_dataset[[\"track_id\", \"midi_id\", \"midi_filepath\",\n",
    "                                   \"title\", \"artist\", \"year\"]]\n",
    "grouped_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7892095",
   "metadata": {},
   "source": [
    "### Adding the genre tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(tagtraum_df, how=\"inner\", left_on=\"track_id\", right_on=\"MSD_name\")\n",
    "dataset = dataset.drop(columns=[\"MSD_name\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dataset = grouped_dataset.merge(tagtraum_df, how=\"inner\", left_on=\"track_id\", right_on=\"MSD_name\")\n",
    "grouped_dataset = grouped_dataset.drop(columns=[\"MSD_name\"])\n",
    "grouped_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c3fb3",
   "metadata": {},
   "source": [
    "## Sluggifying our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = dataset[\"genre\"].unique()\n",
    "artists = dataset[\"artist\"].unique()\n",
    "years = dataset[\"year\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slug(text: str) -> str:\n",
    "    \"\"\"Return an ALL_CAPS alnum/underscore version of `text`.\"\"\"\n",
    "    # 1) strip accents â†’ ascii\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode()\n",
    "    # 2) replace nonâ€‘alnum with underscore\n",
    "    text = re.sub(r\"[^\\w]+\", \"_\", text)\n",
    "    # 3) collapse multiple underscores and upperâ€‘case\n",
    "    return re.sub(r\"_+\", \"_\", text).strip(\"_\").upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_slugged = np.array([slug(genre) for genre in genres])\n",
    "artists_slugged = np.array([slug(artist) for artist in artists])\n",
    "years = np.array([int(year) for year in years if not pd.isna(year)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = pd.DataFrame({\n",
    "    'genre': genres,\n",
    "    'slugged_genre': genres_slugged\n",
    "})\n",
    "\n",
    "artists = pd.DataFrame({\n",
    "    'artist': artists,\n",
    "    'slugged_artist': artists_slugged\n",
    "})\n",
    "\n",
    "years = pd.DataFrame({\n",
    "    'year': years\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b674d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = genres.sort_values(by='genre')\n",
    "artists = artists.sort_values(by='artist')\n",
    "years = years.sort_values(by='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ccfe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"slugged_genre\"] = dataset[\"genre\"].map(genres.set_index('genre')['slugged_genre'])\n",
    "dataset[\"slugged_artist\"] = dataset[\"artist\"].map(artists.set_index('artist')['slugged_artist'])\n",
    "\n",
    "grouped_dataset[\"slugged_genre\"] = grouped_dataset[\"genre\"].map(genres.set_index('genre')['slugged_genre'])\n",
    "grouped_dataset[\"slugged_artist\"] = grouped_dataset[\"artist\"].map(artists.set_index('artist')['slugged_artist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b40cc",
   "metadata": {},
   "source": [
    "## Saving our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace99ee8",
   "metadata": {},
   "source": [
    "### Saving the metadata datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"data/metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab276c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dataset.to_csv(\"data/grouped_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb416e1",
   "metadata": {},
   "source": [
    "### Saving the different parameters to csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres.to_csv(\"data/genres.csv\", index=False)\n",
    "artists.to_csv(\"data/artists.csv\", index=False)\n",
    "years.to_csv(\"data/years.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7a874",
   "metadata": {},
   "source": [
    "# 2. Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/metadata.csv\")\n",
    "grouped_dataset = pd.read_csv(\"data/grouped_metadata.csv\")\n",
    "\n",
    "genres = pd.read_csv(\"data/genres.csv\")\n",
    "titles = pd.read_csv(\"data/titles.csv\")\n",
    "artists = pd.read_csv(\"data/artists.csv\")\n",
    "years = pd.read_csv(\"data/years.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee859144",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_slugged = genres[\"slugged_genre\"].values\n",
    "artists_slugged = artists[\"slugged_artist\"].values\n",
    "years_vals = years[\"year\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d9cd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256/2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbd835ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config whith which the model was trained\n",
    "MAX_TOKENS = 128\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "D_MODEL    = 128\n",
    "N_LAYERS   = 1\n",
    "N_HEADS    = 1\n",
    "\n",
    "# New config to try\n",
    "# MAX_TOKENS = 512\n",
    "# BATCH_SIZE = 2\n",
    "\n",
    "# D_MODEL = 512\n",
    "# N_LAYERS = 6\n",
    "# N_HEADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842c743",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc36dc3e",
   "metadata": {},
   "source": [
    "### Defining the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = TokenizerConfig(\n",
    "#     pitch_range=(21, 108),           # A0â€“C8\n",
    "#     beat_res={(0, 4): 8, (4, 8): 4}, # finer grid in 1st halfâ€‘bar\n",
    "#     num_velocities=32,\n",
    "#     use_rests=True,\n",
    "#     rest_range=(2, 8),               # long rests allowed\n",
    "#     use_tempos=True,\n",
    "#     use_chords=False,\n",
    "#     use_time_signatures=False,\n",
    "#     # you can still add / remove special tokens later with\n",
    "#     # tokenizer.add_to_vocab([...])\n",
    "# )\n",
    "config = TokenizerConfig(num_velocities=16, use_chords=True, use_programs=True)\n",
    "\n",
    "tokenizer = REMI(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ffaa7",
   "metadata": {},
   "source": [
    "### Adding our special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae9165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_toks = \\\n",
    "    [f\"<GENRE_{g}>\"  for g in genres_slugged] + \\\n",
    "        [f\"<ARTIST_{a}>\" for a in artists_slugged] + \\\n",
    "            [f\"<YEAR_{y}>\"   for y in years_vals]  + \\\n",
    "                [\"<EOS>\", \"<PAD>\"]\n",
    "\n",
    "for tok in special_toks:\n",
    "    tokenizer.add_to_vocab(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638cbf00",
   "metadata": {},
   "source": [
    "### Tokenizing: Storing each track as a numpy int32 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb3ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b889b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 1. Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_prefix(genre, artist, year, tokenizer):\n",
    "    \"\"\"Convert metadata row â†’ list[int] conditioning tokens.\"\"\"\n",
    "    genre_tok  = f\"<GENRE_{genre}>\"\n",
    "    artist_tok = f\"<ARTIST_{artist}>\"\n",
    "    year_tok   = f\"<YEAR_{year}>\"\n",
    "\n",
    "    # NOTE: use tokenizer.vocab[...]  (or .token_to_id(...))\n",
    "    return [\n",
    "        tokenizer.vocab[genre_tok],\n",
    "        tokenizer.vocab[artist_tok],\n",
    "        tokenizer.vocab[year_tok],\n",
    "    ]\n",
    "\n",
    "# â”€â”€â”€ 3. Output directory -------------------------------------------------\n",
    "out_dir = \"data/tokens/train\"\n",
    "\n",
    "# â”€â”€â”€ 4. Iterate files ----------------------------------------------------\n",
    "if tokenizing:\n",
    "    rows, _ = grouped_dataset.shape\n",
    "    for row in tqdm(range(1000)):\n",
    "        try:\n",
    "            # 4.0. Get row\n",
    "            row = grouped_dataset.iloc[row]\n",
    "\n",
    "            # 4.1. Get MIDI filepath\n",
    "            midi_path = row[\"midi_filepath\"]\n",
    "\n",
    "            # 4.2. Get the track ID\n",
    "            track_id = row[\"track_id\"]\n",
    "\n",
    "            # 4a. Build CONDITIONING prefix\n",
    "            genre = row[\"slugged_genre\"]\n",
    "            artist = row[\"slugged_artist\"]\n",
    "            year = row[\"year\"]\n",
    "            prefix_ids = build_prefix(genre, artist, year, tokenizer)          # list[int]\n",
    "\n",
    "            # 4b. Encode MIDI to tokens\n",
    "            midi = Score(midi_path)\n",
    "            midi_tokens = tokenizer(midi)                 # list[int]\n",
    "\n",
    "            # 4c. Concatenate prefix + midi + <EOS>\n",
    "            seq_ids = prefix_ids + midi_tokens.ids + [tokenizer.vocab[\"<EOS>\"]]\n",
    "\n",
    "            # 4d. Save as int32 .npy\n",
    "            np.save(f\"{out_dir}/{track_id}.npy\", np.array(seq_ids, dtype=np.int32))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {midi_path}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff6d76",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "991458c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal *relativeâ€‘style* positional encoding.\n",
    "    The tensor it returns has the same shape as `x`\n",
    "    so you can just add it:  x + pos(x)\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    d_model : int            # embedding size\n",
    "    max_len : int, optional  # maximum sequence length\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 2048):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Create the (max_len, d_model) sinusoid table once\n",
    "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float)\n",
    "            * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, d_model)          # (L, D)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Register as a buffer so it moves with .to(device)\n",
    "        self.register_buffer(\"pe\", pe)              # (L, D)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor, shape (batch, seq_len, d_model)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pos : Tensor, same shape as `x`\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds max_len {self.max_len}\")\n",
    "        # (1, L, D) â€“ broadcast over batch dimension\n",
    "        return self.pe[:seq_len].unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c482150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block that merges causal + pad masking into a (BÃ—H, L, L) float mask,\n",
    "    so no hidden boolâ†’float blow-ups occur.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        max_len: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim   = d_model,\n",
    "            num_heads   = n_heads,\n",
    "            dropout     = dropout,\n",
    "            batch_first = True,\n",
    "        )\n",
    "        self.ln1      = nn.LayerNorm(d_model)\n",
    "        self.ln2      = nn.LayerNorm(d_model)\n",
    "        self.ff       = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "\n",
    "        # Precompute float causal mask: 0 on/under diag, -inf above\n",
    "        causal = torch.triu(\n",
    "            torch.full((max_len, max_len), float(\"-inf\")),\n",
    "            diagonal=1\n",
    "        )\n",
    "        self.register_buffer(\"causal_mask\", causal, persistent=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,            # (B, L, D)\n",
    "        pad_mask: torch.Tensor=None  # (B, L), True=keep token, False=pad\n",
    "    ) -> torch.Tensor:\n",
    "        B, L, _ = x.shape\n",
    "        H       = self.self_attn.num_heads\n",
    "        device  = x.device\n",
    "        dtype   = x.dtype\n",
    "\n",
    "        # 1) slice the (LÃ—L) causal mask\n",
    "        causal = self.causal_mask[:L, :L]              # float32, (L, L)\n",
    "\n",
    "        # 2) build a (B, L) float pad mask: 0 on tokens, -inf on pads\n",
    "        if pad_mask is not None:\n",
    "            pad_float = torch.zeros((B, L), device=device, dtype=dtype)\n",
    "            pad_float = pad_float.masked_fill(~pad_mask, float(\"-inf\"))\n",
    "            # 3) expand pad_float to (B, L, L) and add causal\n",
    "            #    pad_float.unsqueeze(1): (B, 1, L) â†’ broadcast over src_len\n",
    "            attn_batch = causal.unsqueeze(0) + pad_float.unsqueeze(1)  # (B, L, L)\n",
    "        else:\n",
    "            attn_batch = causal                               # (L, L)\n",
    "\n",
    "        # 4) if we have a batch, repeat per-head to (BÃ—H, L, L)\n",
    "        if pad_mask is not None:\n",
    "            # attn_batch: (B, L, L) â†’ repeat each batch H times\n",
    "            attn_mask = attn_batch.repeat_interleave(H, dim=0)  # (B*H, L, L)\n",
    "        else:\n",
    "            attn_mask = attn_batch   # 2D mask\n",
    "\n",
    "        # 5) self-attention with ONLY attn_mask\n",
    "        attn_out, _ = self.self_attn(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "\n",
    "        # 6) residual + norm + feed-forward + norm\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "        x = self.ln2(x + self.dropout(self.ff(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdd572fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synphony(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, n_layers=6, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = RelativePositionalEncoding(d_model, max_len=2048)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerDecoderBlock(d_model, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.embed(x) + self.pos(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, pad_mask)\n",
    "        x = self.ln(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1fddf",
   "metadata": {},
   "source": [
    "## The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc5ec0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "random.seed(42)  # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11aca1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_paths = []\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk('data/tokens/train'):\n",
    "    for file in filenames:\n",
    "        full_path = os.path.join(dirpath, file)\n",
    "        if full_path.endswith('.npy'):\n",
    "            tok_paths.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc6d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(tok_paths) * 0.8)  # 80% train, 20% test\n",
    "random.shuffle(tok_paths)\n",
    "\n",
    "train_paths = tok_paths[:split_index]\n",
    "test_paths = tok_paths[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dbd638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val PPL 2556.37\n",
      "EpochÂ 01 â–¸ trainâ€¯PPLÂ 3392.84 | valâ€¯PPLÂ 2556.37\n",
      "  âœ“ new best model saved\n",
      "val PPL 894.69\n",
      "EpochÂ 02 â–¸ trainâ€¯PPLÂ 1805.45 | valâ€¯PPLÂ 894.69\n",
      "  âœ“ new best model saved\n",
      "val PPL 253.51\n",
      "EpochÂ 03 â–¸ trainâ€¯PPLÂ 614.73 | valâ€¯PPLÂ 253.51\n",
      "  âœ“ new best model saved\n",
      "val PPL 122.00\n",
      "EpochÂ 04 â–¸ trainâ€¯PPLÂ 260.41 | valâ€¯PPLÂ 122.00\n",
      "  âœ“ new best model saved\n",
      "val PPL  80.88\n",
      "EpochÂ 05 â–¸ trainâ€¯PPLÂ 166.63 | valâ€¯PPLÂ  80.88\n",
      "  âœ“ new best model saved\n",
      "val PPL  54.68\n",
      "EpochÂ 06 â–¸ trainâ€¯PPLÂ 118.50 | valâ€¯PPLÂ  54.68\n",
      "  âœ“ new best model saved\n",
      "val PPL  39.18\n",
      "EpochÂ 07 â–¸ trainâ€¯PPLÂ  87.33 | valâ€¯PPLÂ  39.18\n",
      "  âœ“ new best model saved\n",
      "val PPL  32.41\n",
      "EpochÂ 08 â–¸ trainâ€¯PPLÂ  70.43 | valâ€¯PPLÂ  32.41\n",
      "  âœ“ new best model saved\n",
      "val PPL  27.63\n",
      "EpochÂ 09 â–¸ trainâ€¯PPLÂ  58.88 | valâ€¯PPLÂ  27.63\n",
      "  âœ“ new best model saved\n",
      "val PPL  24.41\n",
      "EpochÂ 10 â–¸ trainâ€¯PPLÂ  53.12 | valâ€¯PPLÂ  24.41\n",
      "  âœ“ new best model saved\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ 1. Dataset + collate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class MidiTokenDataset(Dataset):\n",
    "    def __init__(self, npy_paths):\n",
    "        self.paths = npy_paths\n",
    "\n",
    "    def __len__(self):               # number of songs in split\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):      # returns 1â€‘D np.ndarray[int]\n",
    "        return np.load(self.paths[idx]).astype(np.int64)\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    B, L = len(batch), MAX_TOKENS\n",
    "    x = torch.full((B, L), pad_id, dtype=torch.long)\n",
    "    for i, seq in enumerate(batch):\n",
    "        seq = torch.from_numpy(seq)\n",
    "        if seq.numel() > L:\n",
    "            start = torch.randint(0, seq.numel() - L + 1, (1,)).item()\n",
    "            seq = seq[start : start + L]\n",
    "        x[i, : seq.numel()] = seq\n",
    "    pad_mask = ~x.eq(pad_id)\n",
    "    return x, pad_mask\n",
    "\n",
    "\n",
    "# â”€â”€â”€ 2. DataLoaders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PAD_ID = tokenizer.vocab['<PAD>']          # or use the ID you chose for <PAD>\n",
    "\n",
    "train_ds = MidiTokenDataset(train_paths)\n",
    "val_ds   = MidiTokenDataset(test_paths)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=lambda b: collate_fn(b, PAD_ID)\n",
    ")\n",
    "val_loader   = DataLoader(\n",
    "    val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "    collate_fn=lambda b: collate_fn(b, PAD_ID)\n",
    ")\n",
    "\n",
    "# â”€â”€â”€ 3. Model, optimiser, scheduler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "model = Synphony(\n",
    "    vocab_size=len(tokenizer), d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS, n_heads=N_HEADS).to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optim, max_lr=1e-4,\n",
    "    steps_per_epoch=len(train_loader), epochs=50\n",
    ")\n",
    "\n",
    "\n",
    "# â”€â”€â”€ 4. Training loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, 11):                         # 50Â epochs\n",
    "    # ---- train ----------------------------------------------------------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, pad_mask in train_loader:          # pad_mask: (B, L)\n",
    "        x, pad_mask = x.to(device), pad_mask.to(device)\n",
    "\n",
    "        logits = model(x[:, :-1], pad_mask=pad_mask[:, :-1])\n",
    "\n",
    "        loss   = F.cross_entropy(\n",
    "            logits.reshape(-1, logits.size(-1)),\n",
    "            x[:, 1:].reshape(-1),\n",
    "            ignore_index=PAD_ID,\n",
    "            label_smoothing=0.1\n",
    "        )\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step(); sched.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_ppl = math.exp(running_loss / len(train_loader))\n",
    "\n",
    "    # ---- validation -----------------------------------------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, pad_mask in val_loader:             # pad_mask is (B, L)\n",
    "            x, pad_mask = x.to(device), pad_mask.to(device)\n",
    "\n",
    "            # exactly like in training\n",
    "            logits  = model(x[:, :-1], pad_mask=pad_mask[:, :-1])\n",
    "            val_loss += F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                x[:, 1:].reshape(-1),\n",
    "                ignore_index=PAD_ID\n",
    "            ).item()\n",
    "\n",
    "    val_ppl = math.exp(val_loss / len(val_loader))\n",
    "    print(f\"val PPL {val_ppl:6.2f}\")\n",
    "    print(f\"EpochÂ {epoch:02d} â–¸ trainâ€¯PPLÂ {train_ppl:6.2f} | valâ€¯PPLÂ {val_ppl:6.2f}\")\n",
    "\n",
    "    # ---- checkpoint -----------------------------------------------------\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"synphony_best.pt\")\n",
    "        print(\"  âœ“ new best model saved\")\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c49a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02358a98",
   "metadata": {},
   "source": [
    "# 3. Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfd5b917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synphony(\n",
       "  (embed): Embedding(3429, 128)\n",
       "  (pos): RelativePositionalEncoding()\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerDecoderBlock(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=128, out_features=3429, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 1.0\n",
    "TOP_K = 8\n",
    "\n",
    "# â”€â”€â”€ 2. Helper for top-k filtering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def top_k_logits(logits, k):\n",
    "    v, _ = torch.topk(logits, k)\n",
    "    threshold = v[-1]\n",
    "    return torch.where(logits < threshold, torch.full_like(logits, -float(\"Inf\")), logits)\n",
    "\n",
    "# â”€â”€â”€ 3. Autoregressive generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "        genre:str,\n",
    "        artist:str,\n",
    "        year:int,\n",
    "        max_length:int = MAX_TOKENS\n",
    "    ) -> list[int]:\n",
    "    prefix = build_prefix(genre, artist, year, tokenizer)\n",
    "    input_ids = torch.tensor([prefix], device=device)  # (1, P)\n",
    "    pad_mask  = torch.ones_like(input_ids, dtype=torch.bool, device=device)\n",
    "\n",
    "    for _ in tqdm(range(max_length - len(prefix))):\n",
    "        logits = model(input_ids, pad_mask=pad_mask)\n",
    "        next_logits = logits[0, -1, :]                  # (V,)\n",
    "        next_logits = next_logits / TEMPERATURE\n",
    "        next_logits = top_k_logits(next_logits, TOP_K)\n",
    "        probs = F.softmax(next_logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)  # (1,)\n",
    "        if next_id.item() == tokenizer.vocab[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "        # append and extend pad_mask\n",
    "        input_ids = torch.cat([input_ids, next_id.unsqueeze(0)], dim=1)   # (1, L+1)\n",
    "        pad_mask  = torch.ones_like(input_ids, dtype=torch.bool, device=device)\n",
    "\n",
    "    return input_ids[0].tolist()\n",
    "\n",
    "# â”€â”€â”€ 4. Decode to MIDI & save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def tokens_to_midi(token_ids: list[int], out_path: str):\n",
    "    \"\"\"\n",
    "    Drop the 3 metadata tokens + optional EOS, then decode the rest.\n",
    "    \"\"\"\n",
    "    # 1) drop the first 3 prefix IDs (genre, artist, year)\n",
    "    musical_ids = token_ids[3:]\n",
    "    # 2) drop trailing <EOS> if present\n",
    "    eos_id = tokenizer.vocab[\"<EOS>\"]\n",
    "    if len(musical_ids) > 0 and musical_ids[-1] == eos_id:\n",
    "        musical_ids = musical_ids[:-1]\n",
    "\n",
    "    # 3) decode only the musical tokens back to a PrettyMIDI\n",
    "    pm = tokenizer(musical_ids)\n",
    "    # 4) write out the .mid file\n",
    "    pm.dump_midi(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6fd5313",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m artist_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRICK_ASTLEY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m year_input   \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1987\u001b[39m\n\u001b[0;32m----> 7\u001b[0m gen_ids \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m(genre_input, artist_input, year_input)\n\u001b[1;32m      8\u001b[0m out_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated.mid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m tokens_to_midi(gen_ids, out_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate' is not defined"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€ 5. Run it! â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Example user inputs\n",
    "genre_input  = \"POP\"\n",
    "artist_input = \"RICK_ASTLEY\"\n",
    "year_input   = 1987\n",
    "\n",
    "gen_ids = generate(genre_input, artist_input, year_input)\n",
    "out_file = \"generated.mid\"\n",
    "tokens_to_midi(gen_ids, out_file)\n",
    "print(f\"ðŸŽ¹ Wrote MIDI to {out_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9eb0d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synphony",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
